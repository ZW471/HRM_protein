{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-26T06:05:05.197901Z",
     "start_time": "2025-09-26T06:05:03.767299Z"
    }
   },
   "source": [
    "from flash_attn.flash_attn_triton import flash_attn_func\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        scores = (torch.bmm(q, k.transpose(-2, -1)))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "        # print(self.softmax(\n",
    "        #     scores / torch.sqrt(torch.tensor(k.shape[-1]))\n",
    "        # ))\n",
    "        return torch.bmm(self.softmax(\n",
    "            scores / torch.sqrt(torch.tensor(k.shape[-1]))\n",
    "        ), v)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_qk, dim_v, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.W_q = nn.Linear(dim_in, dim_qk, bias=False)\n",
    "        self.W_k = nn.Linear(dim_in, dim_qk, bias=False)\n",
    "        self.W_v = nn.Linear(dim_in, dim_v, bias=False)\n",
    "        self.attention = Attention()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        print(f\"q: {q.shape}, k: {k.shape}, v: {v.shape}\")\n",
    "\n",
    "        return self.attention(q, k, v, mask=mask)\n",
    "\n",
    "def causal_mask(T: int, device=None, dtype=torch.bool):\n",
    "    # True above the diagonal â‡’ blocked\n",
    "    return torch.triu(torch.ones(T, T, dtype=dtype, device=device), diagonal=1).bool()\n",
    "\n",
    "att = ScaledDotProductAttention(\n",
    "    dim_in=8,\n",
    "    dim_qk=6,\n",
    "    dim_v=4,\n",
    ")\n",
    "\n",
    "sample_x = torch.randn(4, 5, 8)\n",
    "res = att(sample_x, mask=causal_mask(sample_x.shape[-2]))\n",
    "print(f\"res: {res}, shape: {res.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: torch.Size([4, 5, 6]), k: torch.Size([4, 5, 6]), v: torch.Size([4, 5, 4])\n",
      "res: tensor([[[ 0.4613,  0.2435, -1.5595,  0.8940],\n",
      "         [ 0.3192,  0.1920, -0.6335,  0.6146],\n",
      "         [-0.1549, -0.0481, -1.1615,  0.5916],\n",
      "         [-0.1328,  0.0587, -0.6640,  0.1650],\n",
      "         [-0.2915,  0.3614, -0.7433,  0.1308]],\n",
      "\n",
      "        [[ 0.3559, -0.0821, -1.0970, -0.2304],\n",
      "         [-0.0574,  0.0532, -0.8686, -0.2913],\n",
      "         [-0.1693,  0.1506, -0.8161, -0.2438],\n",
      "         [-0.3403,  0.0036, -0.4348, -0.2176],\n",
      "         [-0.3482,  0.0253, -0.1736, -0.1784]],\n",
      "\n",
      "        [[-0.6685, -0.0771, -0.5147, -0.3106],\n",
      "         [-0.5942,  0.0873, -0.0999, -0.1095],\n",
      "         [ 0.8770,  0.2911,  0.9244,  0.0208],\n",
      "         [-0.4599,  0.2917,  0.1165, -0.2777],\n",
      "         [-0.2725,  0.2296,  0.0399, -0.2364]],\n",
      "\n",
      "        [[ 0.6283,  0.6459,  2.0024, -0.0666],\n",
      "         [ 0.4360,  0.1287,  1.1921,  0.3967],\n",
      "         [-0.7163,  0.2043,  0.3544,  0.2952],\n",
      "         [-0.3025,  0.3566,  0.5062,  0.1068],\n",
      "         [-0.1466,  0.2012,  0.0719,  0.1022]]], grad_fn=<BmmBackward0>), shape: torch.Size([4, 5, 4])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T06:51:31.378512Z",
     "start_time": "2025-09-26T06:51:31.368120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_qk, dim_v, dim_out, num_heads, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.W_q = nn.ModuleList([nn.Linear(dim_in, dim_qk, bias=False) for _ in range(num_heads)])\n",
    "        self.W_k = nn.ModuleList([nn.Linear(dim_in, dim_qk, bias=False) for _ in range(num_heads)])\n",
    "        self.W_v = nn.ModuleList([nn.Linear(dim_in, dim_v, bias=False) for _ in range(num_heads)])\n",
    "\n",
    "        self.W_out = nn.Linear(dim_v * num_heads, dim_out, bias=False)\n",
    "\n",
    "        self.attention = Attention()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        V = []\n",
    "        for W_q, W_k, W_v in zip(self.W_q, self.W_k, self.W_v):\n",
    "            v_head = self.attention(W_q(x), W_k(x), W_v(x), mask=mask)\n",
    "            V.append(v_head)\n",
    "\n",
    "        return self.W_out(torch.concat(V, dim=-1))\n",
    "\n",
    "mha = MultiHeadAttention(\n",
    "    dim_in=8,\n",
    "    dim_qk=6,\n",
    "    dim_v=4,\n",
    "    dim_out=10,\n",
    "    num_heads=4,\n",
    ")\n",
    "\n",
    "sample_x = torch.randn(4, 5, 8)\n",
    "result = mha(sample_x)\n",
    "print(f\"shape: {result.shape}\")"
   ],
   "id": "75e491eebd038209",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([4, 5, 10])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T06:51:22.231645Z",
     "start_time": "2025-09-26T06:51:22.215250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "class MultiHeadFlashAttention(nn.Module):\n",
    "    def __init__(self, dim_in, num_heads, dim_head, causal=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.W_q = nn.Linear(dim_in, dim_head * num_heads, bias=False)\n",
    "        self.W_k = nn.Linear(dim_in, dim_head * num_heads, bias=False)\n",
    "        self.W_v = nn.Linear(dim_in, dim_head * num_heads, bias=False)\n",
    "\n",
    "        self.W_out = nn.Linear(dim_head * num_heads, dim_in, bias=False)\n",
    "\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_head\n",
    "        self.dim_in = dim_in\n",
    "        self.causal = causal\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.autocast('cuda'):\n",
    "            b, n = x.shape[:2]\n",
    "            q, k, v = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "            q = q.view(b, n, self.num_heads, self.dim_head)\n",
    "            k = k.view(b, n, self.num_heads, self.dim_head)\n",
    "            v = v.view(b, n, self.num_heads, self.dim_head)\n",
    "\n",
    "            return self.W_out(flash_attn_func(q, k, v, causal=self.causal).view(b, n, -1))\n",
    "\n",
    "mha = MultiHeadFlashAttention(\n",
    "    dim_in=512,\n",
    "    num_heads=8,\n",
    "    dim_head=64\n",
    ").to(\"cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sample_x = torch.randn(4, 5, 512).to(device)\n",
    "result = mha(sample_x)\n",
    "print(f\"shape: {result.shape}\")"
   ],
   "id": "708f31244788e9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([4, 5, 512])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T07:11:05.611812Z",
     "start_time": "2025-09-22T07:11:05.589816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_qk, dim_v, num_heads, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mha = MultiHeadAttention(\n",
    "            dim_in=dim_in,\n",
    "            dim_qk=dim_qk,\n",
    "            dim_v=dim_v,\n",
    "            dim_out=dim_in,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        self.mha_norm = nn.LayerNorm(dim_in)\n",
    "        self.activation = nn.GELU()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_in),\n",
    "            self.activation,\n",
    "            nn.Linear(dim_in, dim_in)\n",
    "        )\n",
    "        self.ff_norm = nn.LayerNorm(dim_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mha_out = self.mha_norm(self.mha(x)) + x\n",
    "        return self.ff_norm(self.ffn(mha_out)) + mha_out\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim_in, dim_qk, dim_v, num_heads, num_layers, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        layers = [TransformerEncoderLayer(dim_in=dim_in, dim_qk=dim_qk, dim_v=dim_v, num_heads=num_heads) for _ in range(num_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = TransformerEncoder(\n",
    "    dim_in=8,\n",
    "    dim_qk=6,\n",
    "    dim_v=4,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    ")\n",
    "\n",
    "sample_x = torch.randn(4, 5, 8)\n",
    "result = model(sample_x)\n",
    "print(f\"shape: {result.shape}, result: {result}\")"
   ],
   "id": "da41fbf2e418cc5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([4, 5, 8]), result: tensor([[[-0.3201,  0.0625,  1.6362,  3.5891, -0.1413, -1.2206, -3.2086,\n",
      "           1.0505],\n",
      "         [ 1.5041, -1.1012,  2.5851,  5.8168, -1.7879, -2.2085, -4.2433,\n",
      "           0.0286],\n",
      "         [ 1.3578, -0.3948,  2.2239,  4.9473, -2.4554, -0.6486, -3.6839,\n",
      "          -2.6097],\n",
      "         [-0.6036, -1.3961, -1.3277,  4.9567,  0.5778, -0.3719, -5.2545,\n",
      "           3.0133],\n",
      "         [-0.6930, -0.3979, -1.5184,  5.1135, -0.1629,  0.2616, -6.4386,\n",
      "           1.7458]],\n",
      "\n",
      "        [[-1.9933,  0.2171,  1.4529,  4.4601, -0.3574, -0.3835, -3.5845,\n",
      "           0.9353],\n",
      "         [ 0.6849, -0.4494,  2.7661,  3.8220, -3.2947,  0.2268, -3.2978,\n",
      "          -2.3724],\n",
      "         [ 0.3149, -2.2079, -0.2901,  5.0354, -0.7963, -0.1709, -6.6625,\n",
      "           2.0712],\n",
      "         [ 1.7548, -1.8341,  1.8345,  4.6398, -2.3439,  0.8804, -4.2612,\n",
      "          -1.5792],\n",
      "         [ 0.6798,  0.6331,  2.6075,  3.2651, -3.4048, -0.1269, -4.0909,\n",
      "          -1.3368]],\n",
      "\n",
      "        [[-1.9374, -0.7035,  1.0690,  1.3978, -1.9009, -0.1401, -2.2731,\n",
      "          -0.3018],\n",
      "         [-1.8971,  0.0785,  1.4216,  2.9788, -0.7848, -0.0413, -3.3762,\n",
      "           0.2755],\n",
      "         [-0.2840, -0.3215,  2.9066,  3.9721, -1.6569,  0.9151, -5.4959,\n",
      "          -2.7276],\n",
      "         [ 0.2443,  1.2469,  2.4715,  2.7979, -1.2785,  0.0254, -4.7023,\n",
      "          -3.0132],\n",
      "         [-3.4283,  0.3615,  0.4423,  4.6756, -1.6840,  1.5547, -5.4629,\n",
      "           0.6818]],\n",
      "\n",
      "        [[ 2.7962, -1.5993,  1.6869,  2.7357, -4.3324,  0.2796, -5.5551,\n",
      "           1.1488],\n",
      "         [ 0.8869, -1.0929,  1.1423,  3.0433, -3.0803,  1.2917, -4.9723,\n",
      "           1.9478],\n",
      "         [ 0.0875, -1.2322,  2.5586,  3.9994, -2.2348,  1.4665, -3.1891,\n",
      "          -0.4310],\n",
      "         [ 3.8680, -2.3878,  1.9107,  2.2689, -4.6149,  1.7517, -4.9300,\n",
      "          -0.5503],\n",
      "         [ 3.4421, -2.4411,  2.0521,  1.5310, -3.8770,  1.0879, -4.0052,\n",
      "          -0.3452]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T07:53:33.804300Z",
     "start_time": "2025-09-22T07:53:33.779876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_qk, dim_v, num_heads, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.masked_mha = MultiHeadAttention(\n",
    "            dim_in=dim_in,\n",
    "            dim_qk=dim_qk,\n",
    "            dim_v=dim_v,\n",
    "            dim_out=dim_in,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.masked_mha_norm = nn.LayerNorm(dim_in)\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            dim_in=dim_in,\n",
    "            dim_qk=dim_qk,\n",
    "            dim_v=dim_v,\n",
    "            dim_out=dim_in,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.mha_norm = nn.LayerNorm(dim_in)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_in),\n",
    "            self.activation,\n",
    "            nn.Linear(dim_in, dim_in)\n",
    "        )\n",
    "        self.ff_norm = nn.LayerNorm(dim_in)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        masked_mha_out = self.masked_mha_norm(self.masked_mha(x, mask=mask)) + x\n",
    "        mha_out = self.mha_norm(self.mha(masked_mha_out)) + masked_mha_out\n",
    "        return self.ff_norm(self.ffn(mha_out)) + mha_out\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, dim_in, dim_qk, dim_v, num_heads, num_layers, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        layers = [TransformerDecoderLayer(dim_in=dim_in, dim_qk=dim_qk, dim_v=dim_v, num_heads=num_heads) for _ in range(num_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "model = TransformerDecoder(\n",
    "    dim_in=8,\n",
    "    dim_qk=6,\n",
    "    dim_v=4,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    ")\n",
    "\n",
    "sample_x = torch.randn(4, 5, 8)\n",
    "result = model(sample_x, mask=causal_mask(sample_x.shape[-2]))\n",
    "print(f\"shape: {result.shape}, result: {result}\")"
   ],
   "id": "7064495d96a308f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([4, 5, 8]), result: tensor([[[-2.1156,  0.3588, -0.1507,  3.9490, -0.3605, -0.3159, -1.5909,\n",
      "          -4.3717],\n",
      "         [ 0.2610, -2.9844,  3.2863,  3.9364,  2.0252, -1.0371, -4.7748,\n",
      "          -1.9584],\n",
      "         [ 2.7874, -2.9923,  3.7135,  4.7617,  4.1527, -1.1137, -4.7065,\n",
      "          -3.9936],\n",
      "         [ 3.7710, -5.5868,  1.4442,  3.5567,  4.2420,  0.3629, -3.0979,\n",
      "          -3.2620],\n",
      "         [ 2.4560, -6.7225,  3.0729,  3.6182,  4.5079, -0.8883, -3.1035,\n",
      "          -4.3668]],\n",
      "\n",
      "        [[ 0.0565, -2.5177,  4.2840,  4.7264, -1.3165,  0.0393,  0.7770,\n",
      "          -4.8648],\n",
      "         [ 2.0736, -5.3310,  5.5753,  2.2674,  1.2505, -5.0134, -1.5506,\n",
      "          -0.4883],\n",
      "         [ 4.6582, -2.4041,  6.4920,  4.5495, -1.9759, -4.1674, -4.6975,\n",
      "          -4.9908],\n",
      "         [ 3.5562, -1.6611,  5.5223,  5.5378, -0.5072, -3.8585, -4.5194,\n",
      "          -5.0245],\n",
      "         [ 5.5365, -2.4497,  4.8449,  4.3338,  1.4866, -2.8502, -3.9409,\n",
      "          -5.0749]],\n",
      "\n",
      "        [[ 2.6910, -4.1993,  2.8822, -0.3716,  5.6385,  0.3238,  0.3245,\n",
      "          -3.8130],\n",
      "         [ 0.9911, -1.7471,  0.8164, -3.0324,  1.5557,  4.2400,  0.4983,\n",
      "          -5.6618],\n",
      "         [ 1.2359, -2.8841,  0.9927, -2.9946,  1.5823,  4.8088,  1.4684,\n",
      "          -3.8226],\n",
      "         [ 1.5145, -2.8645, -0.4592, -1.9412,  2.8909,  0.8307,  3.0090,\n",
      "          -2.0570],\n",
      "         [ 3.8455, -4.6036,  2.9664, -0.5105,  6.3938,  2.9659, -0.4164,\n",
      "          -7.3853]],\n",
      "\n",
      "        [[ 3.3846, -5.1208,  4.6921, -6.1395,  6.3719,  0.6029,  2.9637,\n",
      "          -9.8412],\n",
      "         [ 3.1776, -3.5446,  2.0814, -5.4501,  7.9425, -0.7388,  4.9507,\n",
      "          -7.1180],\n",
      "         [ 0.4229, -4.6246,  2.2408, -1.5371,  5.5275, -1.7414,  3.8195,\n",
      "          -4.8605],\n",
      "         [ 2.0218, -4.4040, -0.5940, -5.2149,  5.8274,  2.5944,  4.3563,\n",
      "          -4.2782],\n",
      "         [ 2.1771, -3.8848,  3.5905, -6.2847,  5.9797,  1.1602,  4.4632,\n",
      "          -7.5962]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e54266d49c0cec8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
